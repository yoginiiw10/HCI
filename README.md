# HCI

# Multimodal HCI System for Education

## Overview

Welcome to the repository for the **Multimodal HCI System for Education** project. Our innovative system is designed to revolutionize language learning by integrating advanced technologies such as face recognition, emotion detection, and dialogue systems. By leveraging the AMI and SEMAINE corpora, our system dynamically adapts to each student's emotional and cognitive states, delivering personalized and engaging feedback. This transformative approach aims to enhance learning outcomes, foster inclusivity, and address critical educational needs, contributing to a more equitable and human-centered learning experience.

## Project Description

Traditional education systems often struggle to adapt to individual students' emotional and cognitive needs, leading to disengagement and suboptimal learning outcomes. This project addresses this gap by developing a Multimodal HCI System for Education that integrates:

- **Face Recognition**: To identify and monitor students’ facial expressions.
- **Emotion Detection**: To understand and respond to students’ emotional states.
- **Advanced Dialogue Systems**: To provide dynamic and context-aware feedback.

The system personalizes learning experiences, particularly in language education, by adapting in real-time to students’ expressions, emotions, and inputs, making education more inclusive, engaging, and effective.

## Key Features

- **Real-Time Adaptation**: The system adjusts learning materials based on students’ emotional and cognitive states.
- **Personalized Feedback**: Provides tailored responses to enhance learning and engagement.
- **Inclusive Learning**: Addresses diverse needs by integrating multiple modalities.
- **Enhanced Learning Outcomes**: Aims to improve student performance and retention.

##Workflow

![image](https://github.com/user-attachments/assets/bbe3c17d-f666-4651-9a8f-56e0053459c2)



## Motivation

Despite advancements in multimodal AI and HCI systems, current educational tools often lack seamless integration across multiple modalities. Most solutions either focus on a single modality or fail to dynamically adapt to students' real-time emotional and cognitive states. The application of these technologies in language learning, particularly with real-time adaptive feedback, remains underexplored. Our project fills this gap by developing a holistic system that incorporates text, voice, and facial expression analysis to enhance personalized learning.

## Technologies Used
- **Corpora**: AMI, SEMAINE

## Acknowledgments

- AMI Corpus
- SEMAINE Corpus
